/*
Copyright 2022 Upbound Inc.
*/

// Code generated by upjet. DO NOT EDIT.

package v1alpha1

import (
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime/schema"

	v1 "github.com/crossplane/crossplane-runtime/apis/common/v1"
)

type AccessInitParameters struct {

	// (Updatable) The OCID of a log group to work with.
	LogGroupID *string `json:"logGroupId,omitempty" tf:"log_group_id,omitempty"`

	// (Updatable) The OCID of a log to work with.
	LogID *string `json:"logId,omitempty" tf:"log_id,omitempty"`
}

type AccessObservation struct {

	// (Updatable) The OCID of a log group to work with.
	LogGroupID *string `json:"logGroupId,omitempty" tf:"log_group_id,omitempty"`

	// (Updatable) The OCID of a log to work with.
	LogID *string `json:"logId,omitempty" tf:"log_id,omitempty"`
}

type AccessParameters struct {

	// (Updatable) The OCID of a log group to work with.
	// +kubebuilder:validation:Optional
	LogGroupID *string `json:"logGroupId" tf:"log_group_id,omitempty"`

	// (Updatable) The OCID of a log to work with.
	// +kubebuilder:validation:Optional
	LogID *string `json:"logId" tf:"log_id,omitempty"`
}

type AutoScalingPoliciesInitParameters struct {

	// (Updatable) The type of autoscaling policy.
	AutoScalingPolicyType *string `json:"autoScalingPolicyType,omitempty" tf:"auto_scaling_policy_type,omitempty"`

	// (Updatable) For a threshold-based autoscaling policy, this value is the initial number of instances to launch in the model deployment immediately after autoscaling is enabled. Note that anytime this value is updated, the number of instances will be reset to this value. After autoscaling retrieves performance metrics, the number of instances is automatically adjusted from this initial number to a number that is based on the limits that you set.
	InitialInstanceCount *float64 `json:"initialInstanceCount,omitempty" tf:"initial_instance_count,omitempty"`

	// (Updatable) For a threshold-based autoscaling policy, this value is the maximum number of instances the model deployment is allowed to increase to (scale out).
	MaximumInstanceCount *float64 `json:"maximumInstanceCount,omitempty" tf:"maximum_instance_count,omitempty"`

	// (Updatable) For a threshold-based autoscaling policy, this value is the minimum number of instances the model deployment is allowed to decrease to (scale in).
	MinimumInstanceCount *float64 `json:"minimumInstanceCount,omitempty" tf:"minimum_instance_count,omitempty"`

	// (Updatable) The list of autoscaling policy rules.
	Rules []RulesInitParameters `json:"rules,omitempty" tf:"rules,omitempty"`
}

type AutoScalingPoliciesObservation struct {

	// (Updatable) The type of autoscaling policy.
	AutoScalingPolicyType *string `json:"autoScalingPolicyType,omitempty" tf:"auto_scaling_policy_type,omitempty"`

	// (Updatable) For a threshold-based autoscaling policy, this value is the initial number of instances to launch in the model deployment immediately after autoscaling is enabled. Note that anytime this value is updated, the number of instances will be reset to this value. After autoscaling retrieves performance metrics, the number of instances is automatically adjusted from this initial number to a number that is based on the limits that you set.
	InitialInstanceCount *float64 `json:"initialInstanceCount,omitempty" tf:"initial_instance_count,omitempty"`

	// (Updatable) For a threshold-based autoscaling policy, this value is the maximum number of instances the model deployment is allowed to increase to (scale out).
	MaximumInstanceCount *float64 `json:"maximumInstanceCount,omitempty" tf:"maximum_instance_count,omitempty"`

	// (Updatable) For a threshold-based autoscaling policy, this value is the minimum number of instances the model deployment is allowed to decrease to (scale in).
	MinimumInstanceCount *float64 `json:"minimumInstanceCount,omitempty" tf:"minimum_instance_count,omitempty"`

	// (Updatable) The list of autoscaling policy rules.
	Rules []RulesObservation `json:"rules,omitempty" tf:"rules,omitempty"`
}

type AutoScalingPoliciesParameters struct {

	// (Updatable) The type of autoscaling policy.
	// +kubebuilder:validation:Optional
	AutoScalingPolicyType *string `json:"autoScalingPolicyType" tf:"auto_scaling_policy_type,omitempty"`

	// (Updatable) For a threshold-based autoscaling policy, this value is the initial number of instances to launch in the model deployment immediately after autoscaling is enabled. Note that anytime this value is updated, the number of instances will be reset to this value. After autoscaling retrieves performance metrics, the number of instances is automatically adjusted from this initial number to a number that is based on the limits that you set.
	// +kubebuilder:validation:Optional
	InitialInstanceCount *float64 `json:"initialInstanceCount" tf:"initial_instance_count,omitempty"`

	// (Updatable) For a threshold-based autoscaling policy, this value is the maximum number of instances the model deployment is allowed to increase to (scale out).
	// +kubebuilder:validation:Optional
	MaximumInstanceCount *float64 `json:"maximumInstanceCount" tf:"maximum_instance_count,omitempty"`

	// (Updatable) For a threshold-based autoscaling policy, this value is the minimum number of instances the model deployment is allowed to decrease to (scale in).
	// +kubebuilder:validation:Optional
	MinimumInstanceCount *float64 `json:"minimumInstanceCount" tf:"minimum_instance_count,omitempty"`

	// (Updatable) The list of autoscaling policy rules.
	// +kubebuilder:validation:Optional
	Rules []RulesParameters `json:"rules" tf:"rules,omitempty"`
}

type AutoScalingPoliciesRulesInitParameters struct {

	// (Updatable) The metric expression for creating the alarm used to trigger autoscaling actions on the model deployment.
	MetricExpressionRuleType *string `json:"metricExpressionRuleType,omitempty" tf:"metric_expression_rule_type,omitempty"`

	// (Updatable) Metric type
	MetricType *string `json:"metricType,omitempty" tf:"metric_type,omitempty"`

	// (Updatable) The scaling configuration for the predefined metric expression rule.
	ScaleInConfiguration []RulesScaleInConfigurationInitParameters `json:"scaleInConfiguration,omitempty" tf:"scale_in_configuration,omitempty"`

	// (Updatable) The scaling configuration for the predefined metric expression rule.
	ScaleOutConfiguration []RulesScaleOutConfigurationInitParameters `json:"scaleOutConfiguration,omitempty" tf:"scale_out_configuration,omitempty"`
}

type AutoScalingPoliciesRulesObservation struct {

	// (Updatable) The metric expression for creating the alarm used to trigger autoscaling actions on the model deployment.
	MetricExpressionRuleType *string `json:"metricExpressionRuleType,omitempty" tf:"metric_expression_rule_type,omitempty"`

	// (Updatable) Metric type
	MetricType *string `json:"metricType,omitempty" tf:"metric_type,omitempty"`

	// (Updatable) The scaling configuration for the predefined metric expression rule.
	ScaleInConfiguration []RulesScaleInConfigurationObservation `json:"scaleInConfiguration,omitempty" tf:"scale_in_configuration,omitempty"`

	// (Updatable) The scaling configuration for the predefined metric expression rule.
	ScaleOutConfiguration []RulesScaleOutConfigurationObservation `json:"scaleOutConfiguration,omitempty" tf:"scale_out_configuration,omitempty"`
}

type AutoScalingPoliciesRulesParameters struct {

	// (Updatable) The metric expression for creating the alarm used to trigger autoscaling actions on the model deployment.
	// +kubebuilder:validation:Optional
	MetricExpressionRuleType *string `json:"metricExpressionRuleType" tf:"metric_expression_rule_type,omitempty"`

	// (Updatable) Metric type
	// +kubebuilder:validation:Optional
	MetricType *string `json:"metricType,omitempty" tf:"metric_type,omitempty"`

	// (Updatable) The scaling configuration for the predefined metric expression rule.
	// +kubebuilder:validation:Optional
	ScaleInConfiguration []RulesScaleInConfigurationParameters `json:"scaleInConfiguration" tf:"scale_in_configuration,omitempty"`

	// (Updatable) The scaling configuration for the predefined metric expression rule.
	// +kubebuilder:validation:Optional
	ScaleOutConfiguration []RulesScaleOutConfigurationParameters `json:"scaleOutConfiguration" tf:"scale_out_configuration,omitempty"`
}

type CategoryLogDetailsInitParameters struct {

	// (Updatable) The log details.
	Access []AccessInitParameters `json:"access,omitempty" tf:"access,omitempty"`

	// (Updatable) The log details.
	Predict []PredictInitParameters `json:"predict,omitempty" tf:"predict,omitempty"`
}

type CategoryLogDetailsObservation struct {

	// (Updatable) The log details.
	Access []AccessObservation `json:"access,omitempty" tf:"access,omitempty"`

	// (Updatable) The log details.
	Predict []PredictObservation `json:"predict,omitempty" tf:"predict,omitempty"`
}

type CategoryLogDetailsParameters struct {

	// (Updatable) The log details.
	// +kubebuilder:validation:Optional
	Access []AccessParameters `json:"access,omitempty" tf:"access,omitempty"`

	// (Updatable) The log details.
	// +kubebuilder:validation:Optional
	Predict []PredictParameters `json:"predict,omitempty" tf:"predict,omitempty"`
}

type EnvironmentConfigurationDetailsInitParameters struct {

	// (Applicable when environment_configuration_type=OCIR_CONTAINER) (Updatable) The container image run CMD as a list of strings. Use CMD as arguments to the ENTRYPOINT or the only command to run in the absence of an ENTRYPOINT. The combined size of CMD and ENTRYPOINT must be less than 2048 bytes.
	Cmd []*string `json:"cmd,omitempty" tf:"cmd,omitempty"`

	// (Applicable when environment_configuration_type=OCIR_CONTAINER) (Updatable) The container image run ENTRYPOINT as a list of strings. Accept the CMD as extra arguments. The combined size of CMD and ENTRYPOINT must be less than 2048 bytes. More information on how CMD and ENTRYPOINT interact are here.
	Entrypoint []*string `json:"entrypoint,omitempty" tf:"entrypoint,omitempty"`

	// (Updatable) The environment configuration type
	EnvironmentConfigurationType *string `json:"environmentConfigurationType,omitempty" tf:"environment_configuration_type,omitempty"`

	// (Updatable) Environment variables to set for the web server container. The size of envVars must be less than 2048 bytes. Key should be under 32 characters. Key should contain only letters, digits and underscore (_) Key should start with a letter. Key should have at least 2 characters. Key should not end with underscore eg. TEST_ Key if added cannot be empty. Value can be empty. No specific size limits on individual Values. But overall environment variables is limited to 2048 bytes. Key can't be reserved Model Deployment environment variables.
	// +mapType=granular
	EnvironmentVariables map[string]*string `json:"environmentVariables,omitempty" tf:"environment_variables,omitempty"`

	// (Applicable when environment_configuration_type=OCIR_CONTAINER) (Updatable) The port on which the container HEALTHCHECK would listen. The port can be anything between 1024 and 65535. The following ports cannot be used 24224, 8446, 8447.
	HealthCheckPort *float64 `json:"healthCheckPort,omitempty" tf:"health_check_port,omitempty"`

	// (Updatable) The full path to the Oracle Container Repository (OCIR) registry, image, and tag in a canonical format. Acceptable format: <region>.ocir.io/<registry>/<image>:<tag> <region>.ocir.io/<registry>/<image>:<tag>@digest
	Image *string `json:"image,omitempty" tf:"image,omitempty"`

	// (Applicable when environment_configuration_type=OCIR_CONTAINER) (Updatable) The digest of the container image. For example, sha256:881303a6b2738834d795a32b4a98eb0e5e3d1cad590a712d1e04f9b2fa90a030
	ImageDigest *string `json:"imageDigest,omitempty" tf:"image_digest,omitempty"`

	// (Applicable when environment_configuration_type=OCIR_CONTAINER) (Updatable) The port on which the web server serving the inference is running. The port can be anything between 1024 and 65535. The following ports cannot be used 24224, 8446, 8447.
	ServerPort *float64 `json:"serverPort,omitempty" tf:"server_port,omitempty"`
}

type EnvironmentConfigurationDetailsObservation struct {

	// (Applicable when environment_configuration_type=OCIR_CONTAINER) (Updatable) The container image run CMD as a list of strings. Use CMD as arguments to the ENTRYPOINT or the only command to run in the absence of an ENTRYPOINT. The combined size of CMD and ENTRYPOINT must be less than 2048 bytes.
	Cmd []*string `json:"cmd,omitempty" tf:"cmd,omitempty"`

	// (Applicable when environment_configuration_type=OCIR_CONTAINER) (Updatable) The container image run ENTRYPOINT as a list of strings. Accept the CMD as extra arguments. The combined size of CMD and ENTRYPOINT must be less than 2048 bytes. More information on how CMD and ENTRYPOINT interact are here.
	Entrypoint []*string `json:"entrypoint,omitempty" tf:"entrypoint,omitempty"`

	// (Updatable) The environment configuration type
	EnvironmentConfigurationType *string `json:"environmentConfigurationType,omitempty" tf:"environment_configuration_type,omitempty"`

	// (Updatable) Environment variables to set for the web server container. The size of envVars must be less than 2048 bytes. Key should be under 32 characters. Key should contain only letters, digits and underscore (_) Key should start with a letter. Key should have at least 2 characters. Key should not end with underscore eg. TEST_ Key if added cannot be empty. Value can be empty. No specific size limits on individual Values. But overall environment variables is limited to 2048 bytes. Key can't be reserved Model Deployment environment variables.
	// +mapType=granular
	EnvironmentVariables map[string]*string `json:"environmentVariables,omitempty" tf:"environment_variables,omitempty"`

	// (Applicable when environment_configuration_type=OCIR_CONTAINER) (Updatable) The port on which the container HEALTHCHECK would listen. The port can be anything between 1024 and 65535. The following ports cannot be used 24224, 8446, 8447.
	HealthCheckPort *float64 `json:"healthCheckPort,omitempty" tf:"health_check_port,omitempty"`

	// (Updatable) The full path to the Oracle Container Repository (OCIR) registry, image, and tag in a canonical format. Acceptable format: <region>.ocir.io/<registry>/<image>:<tag> <region>.ocir.io/<registry>/<image>:<tag>@digest
	Image *string `json:"image,omitempty" tf:"image,omitempty"`

	// (Applicable when environment_configuration_type=OCIR_CONTAINER) (Updatable) The digest of the container image. For example, sha256:881303a6b2738834d795a32b4a98eb0e5e3d1cad590a712d1e04f9b2fa90a030
	ImageDigest *string `json:"imageDigest,omitempty" tf:"image_digest,omitempty"`

	// (Applicable when environment_configuration_type=OCIR_CONTAINER) (Updatable) The port on which the web server serving the inference is running. The port can be anything between 1024 and 65535. The following ports cannot be used 24224, 8446, 8447.
	ServerPort *float64 `json:"serverPort,omitempty" tf:"server_port,omitempty"`
}

type EnvironmentConfigurationDetailsParameters struct {

	// (Applicable when environment_configuration_type=OCIR_CONTAINER) (Updatable) The container image run CMD as a list of strings. Use CMD as arguments to the ENTRYPOINT or the only command to run in the absence of an ENTRYPOINT. The combined size of CMD and ENTRYPOINT must be less than 2048 bytes.
	// +kubebuilder:validation:Optional
	Cmd []*string `json:"cmd,omitempty" tf:"cmd,omitempty"`

	// (Applicable when environment_configuration_type=OCIR_CONTAINER) (Updatable) The container image run ENTRYPOINT as a list of strings. Accept the CMD as extra arguments. The combined size of CMD and ENTRYPOINT must be less than 2048 bytes. More information on how CMD and ENTRYPOINT interact are here.
	// +kubebuilder:validation:Optional
	Entrypoint []*string `json:"entrypoint,omitempty" tf:"entrypoint,omitempty"`

	// (Updatable) The environment configuration type
	// +kubebuilder:validation:Optional
	EnvironmentConfigurationType *string `json:"environmentConfigurationType" tf:"environment_configuration_type,omitempty"`

	// (Updatable) Environment variables to set for the web server container. The size of envVars must be less than 2048 bytes. Key should be under 32 characters. Key should contain only letters, digits and underscore (_) Key should start with a letter. Key should have at least 2 characters. Key should not end with underscore eg. TEST_ Key if added cannot be empty. Value can be empty. No specific size limits on individual Values. But overall environment variables is limited to 2048 bytes. Key can't be reserved Model Deployment environment variables.
	// +kubebuilder:validation:Optional
	// +mapType=granular
	EnvironmentVariables map[string]*string `json:"environmentVariables,omitempty" tf:"environment_variables,omitempty"`

	// (Applicable when environment_configuration_type=OCIR_CONTAINER) (Updatable) The port on which the container HEALTHCHECK would listen. The port can be anything between 1024 and 65535. The following ports cannot be used 24224, 8446, 8447.
	// +kubebuilder:validation:Optional
	HealthCheckPort *float64 `json:"healthCheckPort,omitempty" tf:"health_check_port,omitempty"`

	// (Updatable) The full path to the Oracle Container Repository (OCIR) registry, image, and tag in a canonical format. Acceptable format: <region>.ocir.io/<registry>/<image>:<tag> <region>.ocir.io/<registry>/<image>:<tag>@digest
	// +kubebuilder:validation:Optional
	Image *string `json:"image,omitempty" tf:"image,omitempty"`

	// (Applicable when environment_configuration_type=OCIR_CONTAINER) (Updatable) The digest of the container image. For example, sha256:881303a6b2738834d795a32b4a98eb0e5e3d1cad590a712d1e04f9b2fa90a030
	// +kubebuilder:validation:Optional
	ImageDigest *string `json:"imageDigest,omitempty" tf:"image_digest,omitempty"`

	// (Applicable when environment_configuration_type=OCIR_CONTAINER) (Updatable) The port on which the web server serving the inference is running. The port can be anything between 1024 and 65535. The following ports cannot be used 24224, 8446, 8447.
	// +kubebuilder:validation:Optional
	ServerPort *float64 `json:"serverPort,omitempty" tf:"server_port,omitempty"`
}

type InfrastructureConfigurationDetailsInitParameters struct {

	// (Updatable) The minimum network bandwidth for the model deployment.
	BandwidthMbps *float64 `json:"bandwidthMbps,omitempty" tf:"bandwidth_mbps,omitempty"`

	InfrastructureType *string `json:"infrastructureType,omitempty" tf:"infrastructure_type,omitempty"`

	// (Updatable) The model deployment instance configuration.
	InstanceConfiguration []InstanceConfigurationInitParameters `json:"instanceConfiguration,omitempty" tf:"instance_configuration,omitempty"`

	// (Updatable) The maximum network bandwidth for the model deployment.
	MaximumBandwidthMbps *float64 `json:"maximumBandwidthMbps,omitempty" tf:"maximum_bandwidth_mbps,omitempty"`

	// (Updatable) The scaling policy to apply to each model of the deployment.
	ScalingPolicy []ScalingPolicyInitParameters `json:"scalingPolicy,omitempty" tf:"scaling_policy,omitempty"`
}

type InfrastructureConfigurationDetailsObservation struct {

	// (Updatable) The minimum network bandwidth for the model deployment.
	BandwidthMbps *float64 `json:"bandwidthMbps,omitempty" tf:"bandwidth_mbps,omitempty"`

	InfrastructureType *string `json:"infrastructureType,omitempty" tf:"infrastructure_type,omitempty"`

	// (Updatable) The model deployment instance configuration.
	InstanceConfiguration []InstanceConfigurationObservation `json:"instanceConfiguration,omitempty" tf:"instance_configuration,omitempty"`

	// (Updatable) The maximum network bandwidth for the model deployment.
	MaximumBandwidthMbps *float64 `json:"maximumBandwidthMbps,omitempty" tf:"maximum_bandwidth_mbps,omitempty"`

	// (Updatable) The scaling policy to apply to each model of the deployment.
	ScalingPolicy []ScalingPolicyObservation `json:"scalingPolicy,omitempty" tf:"scaling_policy,omitempty"`
}

type InfrastructureConfigurationDetailsParameters struct {

	// (Updatable) The minimum network bandwidth for the model deployment.
	// +kubebuilder:validation:Optional
	BandwidthMbps *float64 `json:"bandwidthMbps,omitempty" tf:"bandwidth_mbps,omitempty"`

	// +kubebuilder:validation:Optional
	InfrastructureType *string `json:"infrastructureType" tf:"infrastructure_type,omitempty"`

	// (Updatable) The model deployment instance configuration.
	// +kubebuilder:validation:Optional
	InstanceConfiguration []InstanceConfigurationParameters `json:"instanceConfiguration" tf:"instance_configuration,omitempty"`

	// (Updatable) The maximum network bandwidth for the model deployment.
	// +kubebuilder:validation:Optional
	MaximumBandwidthMbps *float64 `json:"maximumBandwidthMbps,omitempty" tf:"maximum_bandwidth_mbps,omitempty"`

	// (Updatable) The scaling policy to apply to each model of the deployment.
	// +kubebuilder:validation:Optional
	ScalingPolicy []ScalingPolicyParameters `json:"scalingPolicy,omitempty" tf:"scaling_policy,omitempty"`
}

type InstanceConfigurationInitParameters struct {

	// (Updatable) The shape used to launch the model deployment instances.
	InstanceShapeName *string `json:"instanceShapeName,omitempty" tf:"instance_shape_name,omitempty"`

	// (Updatable) Details for the model-deployment instance shape configuration.
	ModelDeploymentInstanceShapeConfigDetails []ModelDeploymentInstanceShapeConfigDetailsInitParameters `json:"modelDeploymentInstanceShapeConfigDetails,omitempty" tf:"model_deployment_instance_shape_config_details,omitempty"`

	// (Updatable) The OCID of a Data Science private endpoint.
	PrivateEndpointID *string `json:"privateEndpointId,omitempty" tf:"private_endpoint_id,omitempty"`

	// (Updatable) A model deployment instance is provided with a VNIC for network access.  This specifies the OCID of the subnet to create a VNIC in.  The subnet should be in a VCN with a NAT/SGW gateway for egress.
	SubnetID *string `json:"subnetId,omitempty" tf:"subnet_id,omitempty"`
}

type InstanceConfigurationModelDeploymentInstanceShapeConfigDetailsInitParameters struct {

	// (Updatable) The baseline OCPU utilization for a subcore burstable VM instance. If this attribute is left blank, it will default to BASELINE_1_1. The following values are supported: BASELINE_1_8 - baseline usage is 1/8 of an OCPU. BASELINE_1_2 - baseline usage is 1/2 of an OCPU. BASELINE_1_1 - baseline usage is an entire OCPU. This represents a non-burstable instance.
	CPUBaseline *string `json:"cpuBaseline,omitempty" tf:"cpu_baseline,omitempty"`

	// (Updatable) A model-deployment instance of type VM.Standard.E3.Flex or VM.Standard.E4.Flex allows the memory to be specified with in the range of 6 to 1024 GB. VM.Standard3.Flex memory range is between 6 to 512 GB and VM.Optimized3.Flex memory range is between 6 to 256 GB.
	MemoryInGbs *float64 `json:"memoryInGbs,omitempty" tf:"memory_in_gbs,omitempty"`

	// (Updatable) A model-deployment instance of type VM.Standard.E3.Flex or VM.Standard.E4.Flex allows the ocpu count to be specified with in the range of 1 to 64 ocpu. VM.Standard3.Flex OCPU range is between 1 to 32 ocpu and for VM.Optimized3.Flex OCPU range is 1 to 18 ocpu.
	Ocpus *float64 `json:"ocpus,omitempty" tf:"ocpus,omitempty"`
}

type InstanceConfigurationModelDeploymentInstanceShapeConfigDetailsObservation struct {

	// (Updatable) The baseline OCPU utilization for a subcore burstable VM instance. If this attribute is left blank, it will default to BASELINE_1_1. The following values are supported: BASELINE_1_8 - baseline usage is 1/8 of an OCPU. BASELINE_1_2 - baseline usage is 1/2 of an OCPU. BASELINE_1_1 - baseline usage is an entire OCPU. This represents a non-burstable instance.
	CPUBaseline *string `json:"cpuBaseline,omitempty" tf:"cpu_baseline,omitempty"`

	// (Updatable) A model-deployment instance of type VM.Standard.E3.Flex or VM.Standard.E4.Flex allows the memory to be specified with in the range of 6 to 1024 GB. VM.Standard3.Flex memory range is between 6 to 512 GB and VM.Optimized3.Flex memory range is between 6 to 256 GB.
	MemoryInGbs *float64 `json:"memoryInGbs,omitempty" tf:"memory_in_gbs,omitempty"`

	// (Updatable) A model-deployment instance of type VM.Standard.E3.Flex or VM.Standard.E4.Flex allows the ocpu count to be specified with in the range of 1 to 64 ocpu. VM.Standard3.Flex OCPU range is between 1 to 32 ocpu and for VM.Optimized3.Flex OCPU range is 1 to 18 ocpu.
	Ocpus *float64 `json:"ocpus,omitempty" tf:"ocpus,omitempty"`
}

type InstanceConfigurationModelDeploymentInstanceShapeConfigDetailsParameters struct {

	// (Updatable) The baseline OCPU utilization for a subcore burstable VM instance. If this attribute is left blank, it will default to BASELINE_1_1. The following values are supported: BASELINE_1_8 - baseline usage is 1/8 of an OCPU. BASELINE_1_2 - baseline usage is 1/2 of an OCPU. BASELINE_1_1 - baseline usage is an entire OCPU. This represents a non-burstable instance.
	// +kubebuilder:validation:Optional
	CPUBaseline *string `json:"cpuBaseline,omitempty" tf:"cpu_baseline,omitempty"`

	// (Updatable) A model-deployment instance of type VM.Standard.E3.Flex or VM.Standard.E4.Flex allows the memory to be specified with in the range of 6 to 1024 GB. VM.Standard3.Flex memory range is between 6 to 512 GB and VM.Optimized3.Flex memory range is between 6 to 256 GB.
	// +kubebuilder:validation:Optional
	MemoryInGbs *float64 `json:"memoryInGbs,omitempty" tf:"memory_in_gbs,omitempty"`

	// (Updatable) A model-deployment instance of type VM.Standard.E3.Flex or VM.Standard.E4.Flex allows the ocpu count to be specified with in the range of 1 to 64 ocpu. VM.Standard3.Flex OCPU range is between 1 to 32 ocpu and for VM.Optimized3.Flex OCPU range is 1 to 18 ocpu.
	// +kubebuilder:validation:Optional
	Ocpus *float64 `json:"ocpus,omitempty" tf:"ocpus,omitempty"`
}

type InstanceConfigurationObservation struct {

	// (Updatable) The shape used to launch the model deployment instances.
	InstanceShapeName *string `json:"instanceShapeName,omitempty" tf:"instance_shape_name,omitempty"`

	// (Updatable) Details for the model-deployment instance shape configuration.
	ModelDeploymentInstanceShapeConfigDetails []ModelDeploymentInstanceShapeConfigDetailsObservation `json:"modelDeploymentInstanceShapeConfigDetails,omitempty" tf:"model_deployment_instance_shape_config_details,omitempty"`

	// (Updatable) The OCID of a Data Science private endpoint.
	PrivateEndpointID *string `json:"privateEndpointId,omitempty" tf:"private_endpoint_id,omitempty"`

	// (Updatable) A model deployment instance is provided with a VNIC for network access.  This specifies the OCID of the subnet to create a VNIC in.  The subnet should be in a VCN with a NAT/SGW gateway for egress.
	SubnetID *string `json:"subnetId,omitempty" tf:"subnet_id,omitempty"`
}

type InstanceConfigurationParameters struct {

	// (Updatable) The shape used to launch the model deployment instances.
	// +kubebuilder:validation:Optional
	InstanceShapeName *string `json:"instanceShapeName" tf:"instance_shape_name,omitempty"`

	// (Updatable) Details for the model-deployment instance shape configuration.
	// +kubebuilder:validation:Optional
	ModelDeploymentInstanceShapeConfigDetails []ModelDeploymentInstanceShapeConfigDetailsParameters `json:"modelDeploymentInstanceShapeConfigDetails,omitempty" tf:"model_deployment_instance_shape_config_details,omitempty"`

	// (Updatable) The OCID of a Data Science private endpoint.
	// +kubebuilder:validation:Optional
	PrivateEndpointID *string `json:"privateEndpointId,omitempty" tf:"private_endpoint_id,omitempty"`

	// (Updatable) A model deployment instance is provided with a VNIC for network access.  This specifies the OCID of the subnet to create a VNIC in.  The subnet should be in a VCN with a NAT/SGW gateway for egress.
	// +kubebuilder:validation:Optional
	SubnetID *string `json:"subnetId,omitempty" tf:"subnet_id,omitempty"`
}

type ModelConfigurationDetailsInitParameters struct {

	// (Updatable) The minimum network bandwidth for the model deployment.
	BandwidthMbps *float64 `json:"bandwidthMbps,omitempty" tf:"bandwidth_mbps,omitempty"`

	// (Updatable) The model deployment instance configuration.
	InstanceConfiguration []ModelConfigurationDetailsInstanceConfigurationInitParameters `json:"instanceConfiguration,omitempty" tf:"instance_configuration,omitempty"`

	// (Updatable) The maximum network bandwidth for the model deployment.
	MaximumBandwidthMbps *float64 `json:"maximumBandwidthMbps,omitempty" tf:"maximum_bandwidth_mbps,omitempty"`

	// (Updatable) The OCID of the model you want to deploy.
	ModelID *string `json:"modelId,omitempty" tf:"model_id,omitempty"`

	// (Updatable) The scaling policy to apply to each model of the deployment.
	ScalingPolicy []ModelConfigurationDetailsScalingPolicyInitParameters `json:"scalingPolicy,omitempty" tf:"scaling_policy,omitempty"`
}

type ModelConfigurationDetailsInstanceConfigurationInitParameters struct {

	// (Updatable) The shape used to launch the model deployment instances.
	InstanceShapeName *string `json:"instanceShapeName,omitempty" tf:"instance_shape_name,omitempty"`

	// (Updatable) Details for the model-deployment instance shape configuration.
	ModelDeploymentInstanceShapeConfigDetails []InstanceConfigurationModelDeploymentInstanceShapeConfigDetailsInitParameters `json:"modelDeploymentInstanceShapeConfigDetails,omitempty" tf:"model_deployment_instance_shape_config_details,omitempty"`

	// (Updatable) The OCID of a Data Science private endpoint.
	PrivateEndpointID *string `json:"privateEndpointId,omitempty" tf:"private_endpoint_id,omitempty"`

	// (Updatable) A model deployment instance is provided with a VNIC for network access.  This specifies the OCID of the subnet to create a VNIC in.  The subnet should be in a VCN with a NAT/SGW gateway for egress.
	SubnetID *string `json:"subnetId,omitempty" tf:"subnet_id,omitempty"`
}

type ModelConfigurationDetailsInstanceConfigurationObservation struct {

	// (Updatable) The shape used to launch the model deployment instances.
	InstanceShapeName *string `json:"instanceShapeName,omitempty" tf:"instance_shape_name,omitempty"`

	// (Updatable) Details for the model-deployment instance shape configuration.
	ModelDeploymentInstanceShapeConfigDetails []InstanceConfigurationModelDeploymentInstanceShapeConfigDetailsObservation `json:"modelDeploymentInstanceShapeConfigDetails,omitempty" tf:"model_deployment_instance_shape_config_details,omitempty"`

	// (Updatable) The OCID of a Data Science private endpoint.
	PrivateEndpointID *string `json:"privateEndpointId,omitempty" tf:"private_endpoint_id,omitempty"`

	// (Updatable) A model deployment instance is provided with a VNIC for network access.  This specifies the OCID of the subnet to create a VNIC in.  The subnet should be in a VCN with a NAT/SGW gateway for egress.
	SubnetID *string `json:"subnetId,omitempty" tf:"subnet_id,omitempty"`
}

type ModelConfigurationDetailsInstanceConfigurationParameters struct {

	// (Updatable) The shape used to launch the model deployment instances.
	// +kubebuilder:validation:Optional
	InstanceShapeName *string `json:"instanceShapeName,omitempty" tf:"instance_shape_name,omitempty"`

	// (Updatable) Details for the model-deployment instance shape configuration.
	// +kubebuilder:validation:Optional
	ModelDeploymentInstanceShapeConfigDetails []InstanceConfigurationModelDeploymentInstanceShapeConfigDetailsParameters `json:"modelDeploymentInstanceShapeConfigDetails,omitempty" tf:"model_deployment_instance_shape_config_details,omitempty"`

	// (Updatable) The OCID of a Data Science private endpoint.
	// +kubebuilder:validation:Optional
	PrivateEndpointID *string `json:"privateEndpointId,omitempty" tf:"private_endpoint_id,omitempty"`

	// (Updatable) A model deployment instance is provided with a VNIC for network access.  This specifies the OCID of the subnet to create a VNIC in.  The subnet should be in a VCN with a NAT/SGW gateway for egress.
	// +kubebuilder:validation:Optional
	SubnetID *string `json:"subnetId,omitempty" tf:"subnet_id,omitempty"`
}

type ModelConfigurationDetailsObservation struct {

	// (Updatable) The minimum network bandwidth for the model deployment.
	BandwidthMbps *float64 `json:"bandwidthMbps,omitempty" tf:"bandwidth_mbps,omitempty"`

	// (Updatable) The model deployment instance configuration.
	InstanceConfiguration []ModelConfigurationDetailsInstanceConfigurationObservation `json:"instanceConfiguration,omitempty" tf:"instance_configuration,omitempty"`

	// (Updatable) The maximum network bandwidth for the model deployment.
	MaximumBandwidthMbps *float64 `json:"maximumBandwidthMbps,omitempty" tf:"maximum_bandwidth_mbps,omitempty"`

	// (Updatable) The OCID of the model you want to deploy.
	ModelID *string `json:"modelId,omitempty" tf:"model_id,omitempty"`

	// (Updatable) The scaling policy to apply to each model of the deployment.
	ScalingPolicy []ModelConfigurationDetailsScalingPolicyObservation `json:"scalingPolicy,omitempty" tf:"scaling_policy,omitempty"`
}

type ModelConfigurationDetailsParameters struct {

	// (Updatable) The minimum network bandwidth for the model deployment.
	// +kubebuilder:validation:Optional
	BandwidthMbps *float64 `json:"bandwidthMbps,omitempty" tf:"bandwidth_mbps,omitempty"`

	// (Updatable) The model deployment instance configuration.
	// +kubebuilder:validation:Optional
	InstanceConfiguration []ModelConfigurationDetailsInstanceConfigurationParameters `json:"instanceConfiguration,omitempty" tf:"instance_configuration,omitempty"`

	// (Updatable) The maximum network bandwidth for the model deployment.
	// +kubebuilder:validation:Optional
	MaximumBandwidthMbps *float64 `json:"maximumBandwidthMbps,omitempty" tf:"maximum_bandwidth_mbps,omitempty"`

	// (Updatable) The OCID of the model you want to deploy.
	// +kubebuilder:validation:Optional
	ModelID *string `json:"modelId,omitempty" tf:"model_id,omitempty"`

	// (Updatable) The scaling policy to apply to each model of the deployment.
	// +kubebuilder:validation:Optional
	ScalingPolicy []ModelConfigurationDetailsScalingPolicyParameters `json:"scalingPolicy,omitempty" tf:"scaling_policy,omitempty"`
}

type ModelConfigurationDetailsScalingPolicyInitParameters struct {

	// (Updatable) The list of autoscaling policy details.
	AutoScalingPolicies []ScalingPolicyAutoScalingPoliciesInitParameters `json:"autoScalingPolicies,omitempty" tf:"auto_scaling_policies,omitempty"`

	// (Applicable when policy_type=AUTOSCALING) (Updatable) For threshold-based autoscaling policies, this value is the minimum period of time to wait between scaling actions. The cooldown period gives the system time to stabilize before rescaling. The minimum value is 600 seconds, which is also the default. The cooldown period starts when the model deployment becomes ACTIVE after the scaling operation.
	CoolDownInSeconds *float64 `json:"coolDownInSeconds,omitempty" tf:"cool_down_in_seconds,omitempty"`

	// (Updatable) The number of instances for the model deployment.
	InstanceCount *float64 `json:"instanceCount,omitempty" tf:"instance_count,omitempty"`

	// (Applicable when policy_type=AUTOSCALING) (Updatable) Whether the autoscaling policy is enabled.
	IsEnabled *bool `json:"isEnabled,omitempty" tf:"is_enabled,omitempty"`

	// (Updatable) The type of scaling policy.
	PolicyType *string `json:"policyType,omitempty" tf:"policy_type,omitempty"`
}

type ModelConfigurationDetailsScalingPolicyObservation struct {

	// (Updatable) The list of autoscaling policy details.
	AutoScalingPolicies []ScalingPolicyAutoScalingPoliciesObservation `json:"autoScalingPolicies,omitempty" tf:"auto_scaling_policies,omitempty"`

	// (Applicable when policy_type=AUTOSCALING) (Updatable) For threshold-based autoscaling policies, this value is the minimum period of time to wait between scaling actions. The cooldown period gives the system time to stabilize before rescaling. The minimum value is 600 seconds, which is also the default. The cooldown period starts when the model deployment becomes ACTIVE after the scaling operation.
	CoolDownInSeconds *float64 `json:"coolDownInSeconds,omitempty" tf:"cool_down_in_seconds,omitempty"`

	// (Updatable) The number of instances for the model deployment.
	InstanceCount *float64 `json:"instanceCount,omitempty" tf:"instance_count,omitempty"`

	// (Applicable when policy_type=AUTOSCALING) (Updatable) Whether the autoscaling policy is enabled.
	IsEnabled *bool `json:"isEnabled,omitempty" tf:"is_enabled,omitempty"`

	// (Updatable) The type of scaling policy.
	PolicyType *string `json:"policyType,omitempty" tf:"policy_type,omitempty"`
}

type ModelConfigurationDetailsScalingPolicyParameters struct {

	// (Updatable) The list of autoscaling policy details.
	// +kubebuilder:validation:Optional
	AutoScalingPolicies []ScalingPolicyAutoScalingPoliciesParameters `json:"autoScalingPolicies,omitempty" tf:"auto_scaling_policies,omitempty"`

	// (Applicable when policy_type=AUTOSCALING) (Updatable) For threshold-based autoscaling policies, this value is the minimum period of time to wait between scaling actions. The cooldown period gives the system time to stabilize before rescaling. The minimum value is 600 seconds, which is also the default. The cooldown period starts when the model deployment becomes ACTIVE after the scaling operation.
	// +kubebuilder:validation:Optional
	CoolDownInSeconds *float64 `json:"coolDownInSeconds,omitempty" tf:"cool_down_in_seconds,omitempty"`

	// (Updatable) The number of instances for the model deployment.
	// +kubebuilder:validation:Optional
	InstanceCount *float64 `json:"instanceCount,omitempty" tf:"instance_count,omitempty"`

	// (Applicable when policy_type=AUTOSCALING) (Updatable) Whether the autoscaling policy is enabled.
	// +kubebuilder:validation:Optional
	IsEnabled *bool `json:"isEnabled,omitempty" tf:"is_enabled,omitempty"`

	// (Updatable) The type of scaling policy.
	// +kubebuilder:validation:Optional
	PolicyType *string `json:"policyType" tf:"policy_type,omitempty"`
}

type ModelDeploymentConfigurationDetailsInitParameters struct {

	// (Updatable) The type of the model deployment.
	DeploymentType *string `json:"deploymentType,omitempty" tf:"deployment_type,omitempty"`

	// (Updatable) The configuration to carry the environment details thats used in Model Deployment creation
	EnvironmentConfigurationDetails []EnvironmentConfigurationDetailsInitParameters `json:"environmentConfigurationDetails,omitempty" tf:"environment_configuration_details,omitempty"`

	InfrastructureConfigurationDetails []InfrastructureConfigurationDetailsInitParameters `json:"infrastructureConfigurationDetails,omitempty" tf:"infrastructure_configuration_details,omitempty"`

	// (Updatable) The model configuration details.
	ModelConfigurationDetails []ModelConfigurationDetailsInitParameters `json:"modelConfigurationDetails,omitempty" tf:"model_configuration_details,omitempty"`

	ModelGroupConfigurationDetails []ModelGroupConfigurationDetailsInitParameters `json:"modelGroupConfigurationDetails,omitempty" tf:"model_group_configuration_details,omitempty"`
}

type ModelDeploymentConfigurationDetailsObservation struct {

	// (Updatable) The type of the model deployment.
	DeploymentType *string `json:"deploymentType,omitempty" tf:"deployment_type,omitempty"`

	// (Updatable) The configuration to carry the environment details thats used in Model Deployment creation
	EnvironmentConfigurationDetails []EnvironmentConfigurationDetailsObservation `json:"environmentConfigurationDetails,omitempty" tf:"environment_configuration_details,omitempty"`

	InfrastructureConfigurationDetails []InfrastructureConfigurationDetailsObservation `json:"infrastructureConfigurationDetails,omitempty" tf:"infrastructure_configuration_details,omitempty"`

	// (Updatable) The model configuration details.
	ModelConfigurationDetails []ModelConfigurationDetailsObservation `json:"modelConfigurationDetails,omitempty" tf:"model_configuration_details,omitempty"`

	ModelGroupConfigurationDetails []ModelGroupConfigurationDetailsObservation `json:"modelGroupConfigurationDetails,omitempty" tf:"model_group_configuration_details,omitempty"`
}

type ModelDeploymentConfigurationDetailsParameters struct {

	// (Updatable) The type of the model deployment.
	// +kubebuilder:validation:Optional
	DeploymentType *string `json:"deploymentType" tf:"deployment_type,omitempty"`

	// (Updatable) The configuration to carry the environment details thats used in Model Deployment creation
	// +kubebuilder:validation:Optional
	EnvironmentConfigurationDetails []EnvironmentConfigurationDetailsParameters `json:"environmentConfigurationDetails,omitempty" tf:"environment_configuration_details,omitempty"`

	// +kubebuilder:validation:Optional
	InfrastructureConfigurationDetails []InfrastructureConfigurationDetailsParameters `json:"infrastructureConfigurationDetails,omitempty" tf:"infrastructure_configuration_details,omitempty"`

	// (Updatable) The model configuration details.
	// +kubebuilder:validation:Optional
	ModelConfigurationDetails []ModelConfigurationDetailsParameters `json:"modelConfigurationDetails,omitempty" tf:"model_configuration_details,omitempty"`

	// +kubebuilder:validation:Optional
	ModelGroupConfigurationDetails []ModelGroupConfigurationDetailsParameters `json:"modelGroupConfigurationDetails,omitempty" tf:"model_group_configuration_details,omitempty"`
}

type ModelDeploymentInitParameters struct {

	// (Updatable) The log details for each category.
	CategoryLogDetails []CategoryLogDetailsInitParameters `json:"categoryLogDetails,omitempty" tf:"category_log_details,omitempty"`

	// (Updatable) The OCID of the compartment where you want to create the model deployment.
	CompartmentID *string `json:"compartmentId,omitempty" tf:"compartment_id,omitempty"`

	// (Updatable) Defined tags for this resource. Each key is predefined and scoped to a namespace. See Resource Tags. Example: {"Operations.CostCenter": "42"}
	// +mapType=granular
	DefinedTags map[string]*string `json:"definedTags,omitempty" tf:"defined_tags,omitempty"`

	// (Updatable) A short description of the model deployment.
	Description *string `json:"description,omitempty" tf:"description,omitempty"`

	// (Updatable) A user-friendly display name for the resource. Does not have to be unique, and can be modified. Avoid entering confidential information. Example: My ModelDeployment
	DisplayName *string `json:"displayName,omitempty" tf:"display_name,omitempty"`

	// (Updatable) Free-form tags for this resource. Each tag is a simple key-value pair with no predefined name, type, or namespace. See Resource Tags. Example: {"Department": "Finance"}
	// +mapType=granular
	FreeformTags map[string]*string `json:"freeformTags,omitempty" tf:"freeform_tags,omitempty"`

	// (Updatable) The model deployment configuration details.
	ModelDeploymentConfigurationDetails []ModelDeploymentConfigurationDetailsInitParameters `json:"modelDeploymentConfigurationDetails,omitempty" tf:"model_deployment_configuration_details,omitempty"`

	// URL to fetch the Resource Principal Token from the parent resource.
	OpcParentRptURL *string `json:"opcParentRptUrl,omitempty" tf:"opc_parent_rpt_url,omitempty"`

	// The OCID of the project to associate with the model deployment.
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`

	// (Updatable) The target state for the Model Deployment. Could be set to ACTIVE or INACTIVE.
	State *string `json:"state,omitempty" tf:"state,omitempty"`
}

type ModelDeploymentInstanceShapeConfigDetailsInitParameters struct {

	// (Updatable) The baseline OCPU utilization for a subcore burstable VM instance. If this attribute is left blank, it will default to BASELINE_1_1. The following values are supported: BASELINE_1_8 - baseline usage is 1/8 of an OCPU. BASELINE_1_2 - baseline usage is 1/2 of an OCPU. BASELINE_1_1 - baseline usage is an entire OCPU. This represents a non-burstable instance.
	CPUBaseline *string `json:"cpuBaseline,omitempty" tf:"cpu_baseline,omitempty"`

	// (Updatable) A model-deployment instance of type VM.Standard.E3.Flex or VM.Standard.E4.Flex allows the memory to be specified with in the range of 6 to 1024 GB. VM.Standard3.Flex memory range is between 6 to 512 GB and VM.Optimized3.Flex memory range is between 6 to 256 GB.
	MemoryInGbs *float64 `json:"memoryInGbs,omitempty" tf:"memory_in_gbs,omitempty"`

	// (Updatable) A model-deployment instance of type VM.Standard.E3.Flex or VM.Standard.E4.Flex allows the ocpu count to be specified with in the range of 1 to 64 ocpu. VM.Standard3.Flex OCPU range is between 1 to 32 ocpu and for VM.Optimized3.Flex OCPU range is 1 to 18 ocpu.
	Ocpus *float64 `json:"ocpus,omitempty" tf:"ocpus,omitempty"`
}

type ModelDeploymentInstanceShapeConfigDetailsObservation struct {

	// (Updatable) The baseline OCPU utilization for a subcore burstable VM instance. If this attribute is left blank, it will default to BASELINE_1_1. The following values are supported: BASELINE_1_8 - baseline usage is 1/8 of an OCPU. BASELINE_1_2 - baseline usage is 1/2 of an OCPU. BASELINE_1_1 - baseline usage is an entire OCPU. This represents a non-burstable instance.
	CPUBaseline *string `json:"cpuBaseline,omitempty" tf:"cpu_baseline,omitempty"`

	// (Updatable) A model-deployment instance of type VM.Standard.E3.Flex or VM.Standard.E4.Flex allows the memory to be specified with in the range of 6 to 1024 GB. VM.Standard3.Flex memory range is between 6 to 512 GB and VM.Optimized3.Flex memory range is between 6 to 256 GB.
	MemoryInGbs *float64 `json:"memoryInGbs,omitempty" tf:"memory_in_gbs,omitempty"`

	// (Updatable) A model-deployment instance of type VM.Standard.E3.Flex or VM.Standard.E4.Flex allows the ocpu count to be specified with in the range of 1 to 64 ocpu. VM.Standard3.Flex OCPU range is between 1 to 32 ocpu and for VM.Optimized3.Flex OCPU range is 1 to 18 ocpu.
	Ocpus *float64 `json:"ocpus,omitempty" tf:"ocpus,omitempty"`
}

type ModelDeploymentInstanceShapeConfigDetailsParameters struct {

	// (Updatable) The baseline OCPU utilization for a subcore burstable VM instance. If this attribute is left blank, it will default to BASELINE_1_1. The following values are supported: BASELINE_1_8 - baseline usage is 1/8 of an OCPU. BASELINE_1_2 - baseline usage is 1/2 of an OCPU. BASELINE_1_1 - baseline usage is an entire OCPU. This represents a non-burstable instance.
	// +kubebuilder:validation:Optional
	CPUBaseline *string `json:"cpuBaseline,omitempty" tf:"cpu_baseline,omitempty"`

	// (Updatable) A model-deployment instance of type VM.Standard.E3.Flex or VM.Standard.E4.Flex allows the memory to be specified with in the range of 6 to 1024 GB. VM.Standard3.Flex memory range is between 6 to 512 GB and VM.Optimized3.Flex memory range is between 6 to 256 GB.
	// +kubebuilder:validation:Optional
	MemoryInGbs *float64 `json:"memoryInGbs,omitempty" tf:"memory_in_gbs,omitempty"`

	// (Updatable) A model-deployment instance of type VM.Standard.E3.Flex or VM.Standard.E4.Flex allows the ocpu count to be specified with in the range of 1 to 64 ocpu. VM.Standard3.Flex OCPU range is between 1 to 32 ocpu and for VM.Optimized3.Flex OCPU range is 1 to 18 ocpu.
	// +kubebuilder:validation:Optional
	Ocpus *float64 `json:"ocpus,omitempty" tf:"ocpus,omitempty"`
}

type ModelDeploymentObservation struct {

	// (Updatable) The log details for each category.
	CategoryLogDetails []CategoryLogDetailsObservation `json:"categoryLogDetails,omitempty" tf:"category_log_details,omitempty"`

	// (Updatable) The OCID of the compartment where you want to create the model deployment.
	CompartmentID *string `json:"compartmentId,omitempty" tf:"compartment_id,omitempty"`

	// The OCID of the user who created the model deployment.
	CreatedBy *string `json:"createdBy,omitempty" tf:"created_by,omitempty"`

	// (Updatable) Defined tags for this resource. Each key is predefined and scoped to a namespace. See Resource Tags. Example: {"Operations.CostCenter": "42"}
	// +mapType=granular
	DefinedTags map[string]*string `json:"definedTags,omitempty" tf:"defined_tags,omitempty"`

	// (Updatable) A short description of the model deployment.
	Description *string `json:"description,omitempty" tf:"description,omitempty"`

	// (Updatable) A user-friendly display name for the resource. Does not have to be unique, and can be modified. Avoid entering confidential information. Example: My ModelDeployment
	DisplayName *string `json:"displayName,omitempty" tf:"display_name,omitempty"`

	// (Updatable) Free-form tags for this resource. Each tag is a simple key-value pair with no predefined name, type, or namespace. See Resource Tags. Example: {"Department": "Finance"}
	// +mapType=granular
	FreeformTags map[string]*string `json:"freeformTags,omitempty" tf:"freeform_tags,omitempty"`

	// The OCID of the model deployment.
	ID *string `json:"id,omitempty" tf:"id,omitempty"`

	// Details about the state of the model deployment.
	LifecycleDetails *string `json:"lifecycleDetails,omitempty" tf:"lifecycle_details,omitempty"`

	// (Updatable) The model deployment configuration details.
	ModelDeploymentConfigurationDetails []ModelDeploymentConfigurationDetailsObservation `json:"modelDeploymentConfigurationDetails,omitempty" tf:"model_deployment_configuration_details,omitempty"`

	// Model deployment system data.
	ModelDeploymentSystemData []ModelDeploymentSystemDataObservation `json:"modelDeploymentSystemData,omitempty" tf:"model_deployment_system_data,omitempty"`

	// The URL to interact with the model deployment.
	ModelDeploymentURL *string `json:"modelDeploymentUrl,omitempty" tf:"model_deployment_url,omitempty"`

	// URL to fetch the Resource Principal Token from the parent resource.
	OpcParentRptURL *string `json:"opcParentRptUrl,omitempty" tf:"opc_parent_rpt_url,omitempty"`

	// The OCID of the project to associate with the model deployment.
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`

	// (Updatable) The target state for the Model Deployment. Could be set to ACTIVE or INACTIVE.
	State *string `json:"state,omitempty" tf:"state,omitempty"`

	// The date and time the resource was created, in the timestamp format defined by RFC3339. Example: 2019-08-25T21:10:29.41Z
	TimeCreated *string `json:"timeCreated,omitempty" tf:"time_created,omitempty"`
}

type ModelDeploymentParameters struct {

	// (Updatable) The log details for each category.
	// +kubebuilder:validation:Optional
	CategoryLogDetails []CategoryLogDetailsParameters `json:"categoryLogDetails,omitempty" tf:"category_log_details,omitempty"`

	// (Updatable) The OCID of the compartment where you want to create the model deployment.
	// +kubebuilder:validation:Optional
	CompartmentID *string `json:"compartmentId,omitempty" tf:"compartment_id,omitempty"`

	// (Updatable) Defined tags for this resource. Each key is predefined and scoped to a namespace. See Resource Tags. Example: {"Operations.CostCenter": "42"}
	// +kubebuilder:validation:Optional
	// +mapType=granular
	DefinedTags map[string]*string `json:"definedTags,omitempty" tf:"defined_tags,omitempty"`

	// (Updatable) A short description of the model deployment.
	// +kubebuilder:validation:Optional
	Description *string `json:"description,omitempty" tf:"description,omitempty"`

	// (Updatable) A user-friendly display name for the resource. Does not have to be unique, and can be modified. Avoid entering confidential information. Example: My ModelDeployment
	// +kubebuilder:validation:Optional
	DisplayName *string `json:"displayName,omitempty" tf:"display_name,omitempty"`

	// (Updatable) Free-form tags for this resource. Each tag is a simple key-value pair with no predefined name, type, or namespace. See Resource Tags. Example: {"Department": "Finance"}
	// +kubebuilder:validation:Optional
	// +mapType=granular
	FreeformTags map[string]*string `json:"freeformTags,omitempty" tf:"freeform_tags,omitempty"`

	// (Updatable) The model deployment configuration details.
	// +kubebuilder:validation:Optional
	ModelDeploymentConfigurationDetails []ModelDeploymentConfigurationDetailsParameters `json:"modelDeploymentConfigurationDetails,omitempty" tf:"model_deployment_configuration_details,omitempty"`

	// URL to fetch the Resource Principal Token from the parent resource.
	// +kubebuilder:validation:Optional
	OpcParentRptURL *string `json:"opcParentRptUrl,omitempty" tf:"opc_parent_rpt_url,omitempty"`

	// The OCID of the project to associate with the model deployment.
	// +kubebuilder:validation:Optional
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`

	// (Updatable) The target state for the Model Deployment. Could be set to ACTIVE or INACTIVE.
	// +kubebuilder:validation:Optional
	State *string `json:"state,omitempty" tf:"state,omitempty"`
}

type ModelDeploymentSystemDataInitParameters struct {
}

type ModelDeploymentSystemDataObservation struct {

	// This value is the current count of the model deployment instances.
	CurrentInstanceCount *float64 `json:"currentInstanceCount,omitempty" tf:"current_instance_count,omitempty"`

	// The infrastructure type of the model deployment.
	SystemInfraType *string `json:"systemInfraType,omitempty" tf:"system_infra_type,omitempty"`
}

type ModelDeploymentSystemDataParameters struct {
}

type ModelGroupConfigurationDetailsInitParameters struct {

	// The OCID of the model deployment.
	ModelGroupID *string `json:"modelGroupId,omitempty" tf:"model_group_id,omitempty"`
}

type ModelGroupConfigurationDetailsObservation struct {

	// The OCID of the model deployment.
	ModelGroupID *string `json:"modelGroupId,omitempty" tf:"model_group_id,omitempty"`
}

type ModelGroupConfigurationDetailsParameters struct {

	// The OCID of the model deployment.
	// +kubebuilder:validation:Optional
	ModelGroupID *string `json:"modelGroupId,omitempty" tf:"model_group_id,omitempty"`
}

type PredictInitParameters struct {

	// (Updatable) The OCID of a log group to work with.
	LogGroupID *string `json:"logGroupId,omitempty" tf:"log_group_id,omitempty"`

	// (Updatable) The OCID of a log to work with.
	LogID *string `json:"logId,omitempty" tf:"log_id,omitempty"`
}

type PredictObservation struct {

	// (Updatable) The OCID of a log group to work with.
	LogGroupID *string `json:"logGroupId,omitempty" tf:"log_group_id,omitempty"`

	// (Updatable) The OCID of a log to work with.
	LogID *string `json:"logId,omitempty" tf:"log_id,omitempty"`
}

type PredictParameters struct {

	// (Updatable) The OCID of a log group to work with.
	// +kubebuilder:validation:Optional
	LogGroupID *string `json:"logGroupId" tf:"log_group_id,omitempty"`

	// (Updatable) The OCID of a log to work with.
	// +kubebuilder:validation:Optional
	LogID *string `json:"logId" tf:"log_id,omitempty"`
}

type RulesInitParameters struct {

	// (Updatable) The metric expression for creating the alarm used to trigger autoscaling actions on the model deployment.
	MetricExpressionRuleType *string `json:"metricExpressionRuleType,omitempty" tf:"metric_expression_rule_type,omitempty"`

	// (Updatable) Metric type
	MetricType *string `json:"metricType,omitempty" tf:"metric_type,omitempty"`

	// (Updatable) The scaling configuration for the predefined metric expression rule.
	ScaleInConfiguration []ScaleInConfigurationInitParameters `json:"scaleInConfiguration,omitempty" tf:"scale_in_configuration,omitempty"`

	// (Updatable) The scaling configuration for the predefined metric expression rule.
	ScaleOutConfiguration []ScaleOutConfigurationInitParameters `json:"scaleOutConfiguration,omitempty" tf:"scale_out_configuration,omitempty"`
}

type RulesObservation struct {

	// (Updatable) The metric expression for creating the alarm used to trigger autoscaling actions on the model deployment.
	MetricExpressionRuleType *string `json:"metricExpressionRuleType,omitempty" tf:"metric_expression_rule_type,omitempty"`

	// (Updatable) Metric type
	MetricType *string `json:"metricType,omitempty" tf:"metric_type,omitempty"`

	// (Updatable) The scaling configuration for the predefined metric expression rule.
	ScaleInConfiguration []ScaleInConfigurationObservation `json:"scaleInConfiguration,omitempty" tf:"scale_in_configuration,omitempty"`

	// (Updatable) The scaling configuration for the predefined metric expression rule.
	ScaleOutConfiguration []ScaleOutConfigurationObservation `json:"scaleOutConfiguration,omitempty" tf:"scale_out_configuration,omitempty"`
}

type RulesParameters struct {

	// (Updatable) The metric expression for creating the alarm used to trigger autoscaling actions on the model deployment.
	// +kubebuilder:validation:Optional
	MetricExpressionRuleType *string `json:"metricExpressionRuleType" tf:"metric_expression_rule_type,omitempty"`

	// (Updatable) Metric type
	// +kubebuilder:validation:Optional
	MetricType *string `json:"metricType,omitempty" tf:"metric_type,omitempty"`

	// (Updatable) The scaling configuration for the predefined metric expression rule.
	// +kubebuilder:validation:Optional
	ScaleInConfiguration []ScaleInConfigurationParameters `json:"scaleInConfiguration" tf:"scale_in_configuration,omitempty"`

	// (Updatable) The scaling configuration for the predefined metric expression rule.
	// +kubebuilder:validation:Optional
	ScaleOutConfiguration []ScaleOutConfigurationParameters `json:"scaleOutConfiguration" tf:"scale_out_configuration,omitempty"`
}

type RulesScaleInConfigurationInitParameters struct {

	// (Applicable when metric_expression_rule_type=CUSTOM_EXPRESSION | PREDEFINED_EXPRESSION) (Updatable) The value is used for adjusting the count of instances by.
	InstanceCountAdjustment *float64 `json:"instanceCountAdjustment,omitempty" tf:"instance_count_adjustment,omitempty"`

	// (Applicable when metric_expression_rule_type=CUSTOM_EXPRESSION | PREDEFINED_EXPRESSION) (Updatable) The period of time that the condition defined in the alarm must persist before the alarm state changes from "OK" to "FIRING" or vice versa. For example, a value of 5 minutes means that the alarm must persist in breaching the condition for five minutes before the alarm updates its state to "FIRING"; likewise, the alarm must persist in not breaching the condition for five minutes before the alarm updates its state to "OK."
	PendingDuration *string `json:"pendingDuration,omitempty" tf:"pending_duration,omitempty"`

	// (Updatable) The Monitoring Query Language (MQL) expression to evaluate for the alarm. The Alarms feature of the Monitoring service interprets results for each returned time series as Boolean values, where zero represents false and a non-zero value represents true. A true value means that the trigger rule condition has been met. The query must specify a metric, statistic, interval, and trigger rule (threshold or absence). Supported values for interval: 1m-60m (also 1h). You can optionally specify dimensions and grouping functions. Supported grouping functions: grouping(), groupBy().
	Query *string `json:"query,omitempty" tf:"query,omitempty"`

	// (Updatable) The type of scaling configuration.
	ScalingConfigurationType *string `json:"scalingConfigurationType,omitempty" tf:"scaling_configuration_type,omitempty"`

	// (Updatable) A metric value at which the scaling operation will be triggered.
	Threshold *float64 `json:"threshold,omitempty" tf:"threshold,omitempty"`
}

type RulesScaleInConfigurationObservation struct {

	// (Applicable when metric_expression_rule_type=CUSTOM_EXPRESSION | PREDEFINED_EXPRESSION) (Updatable) The value is used for adjusting the count of instances by.
	InstanceCountAdjustment *float64 `json:"instanceCountAdjustment,omitempty" tf:"instance_count_adjustment,omitempty"`

	// (Applicable when metric_expression_rule_type=CUSTOM_EXPRESSION | PREDEFINED_EXPRESSION) (Updatable) The period of time that the condition defined in the alarm must persist before the alarm state changes from "OK" to "FIRING" or vice versa. For example, a value of 5 minutes means that the alarm must persist in breaching the condition for five minutes before the alarm updates its state to "FIRING"; likewise, the alarm must persist in not breaching the condition for five minutes before the alarm updates its state to "OK."
	PendingDuration *string `json:"pendingDuration,omitempty" tf:"pending_duration,omitempty"`

	// (Updatable) The Monitoring Query Language (MQL) expression to evaluate for the alarm. The Alarms feature of the Monitoring service interprets results for each returned time series as Boolean values, where zero represents false and a non-zero value represents true. A true value means that the trigger rule condition has been met. The query must specify a metric, statistic, interval, and trigger rule (threshold or absence). Supported values for interval: 1m-60m (also 1h). You can optionally specify dimensions and grouping functions. Supported grouping functions: grouping(), groupBy().
	Query *string `json:"query,omitempty" tf:"query,omitempty"`

	// (Updatable) The type of scaling configuration.
	ScalingConfigurationType *string `json:"scalingConfigurationType,omitempty" tf:"scaling_configuration_type,omitempty"`

	// (Updatable) A metric value at which the scaling operation will be triggered.
	Threshold *float64 `json:"threshold,omitempty" tf:"threshold,omitempty"`
}

type RulesScaleInConfigurationParameters struct {

	// (Applicable when metric_expression_rule_type=CUSTOM_EXPRESSION | PREDEFINED_EXPRESSION) (Updatable) The value is used for adjusting the count of instances by.
	// +kubebuilder:validation:Optional
	InstanceCountAdjustment *float64 `json:"instanceCountAdjustment,omitempty" tf:"instance_count_adjustment,omitempty"`

	// (Applicable when metric_expression_rule_type=CUSTOM_EXPRESSION | PREDEFINED_EXPRESSION) (Updatable) The period of time that the condition defined in the alarm must persist before the alarm state changes from "OK" to "FIRING" or vice versa. For example, a value of 5 minutes means that the alarm must persist in breaching the condition for five minutes before the alarm updates its state to "FIRING"; likewise, the alarm must persist in not breaching the condition for five minutes before the alarm updates its state to "OK."
	// +kubebuilder:validation:Optional
	PendingDuration *string `json:"pendingDuration,omitempty" tf:"pending_duration,omitempty"`

	// (Updatable) The Monitoring Query Language (MQL) expression to evaluate for the alarm. The Alarms feature of the Monitoring service interprets results for each returned time series as Boolean values, where zero represents false and a non-zero value represents true. A true value means that the trigger rule condition has been met. The query must specify a metric, statistic, interval, and trigger rule (threshold or absence). Supported values for interval: 1m-60m (also 1h). You can optionally specify dimensions and grouping functions. Supported grouping functions: grouping(), groupBy().
	// +kubebuilder:validation:Optional
	Query *string `json:"query,omitempty" tf:"query,omitempty"`

	// (Updatable) The type of scaling configuration.
	// +kubebuilder:validation:Optional
	ScalingConfigurationType *string `json:"scalingConfigurationType,omitempty" tf:"scaling_configuration_type,omitempty"`

	// (Updatable) A metric value at which the scaling operation will be triggered.
	// +kubebuilder:validation:Optional
	Threshold *float64 `json:"threshold,omitempty" tf:"threshold,omitempty"`
}

type RulesScaleOutConfigurationInitParameters struct {

	// (Applicable when metric_expression_rule_type=CUSTOM_EXPRESSION | PREDEFINED_EXPRESSION) (Updatable) The value is used for adjusting the count of instances by.
	InstanceCountAdjustment *float64 `json:"instanceCountAdjustment,omitempty" tf:"instance_count_adjustment,omitempty"`

	// (Applicable when metric_expression_rule_type=CUSTOM_EXPRESSION | PREDEFINED_EXPRESSION) (Updatable) The period of time that the condition defined in the alarm must persist before the alarm state changes from "OK" to "FIRING" or vice versa. For example, a value of 5 minutes means that the alarm must persist in breaching the condition for five minutes before the alarm updates its state to "FIRING"; likewise, the alarm must persist in not breaching the condition for five minutes before the alarm updates its state to "OK."
	PendingDuration *string `json:"pendingDuration,omitempty" tf:"pending_duration,omitempty"`

	// (Updatable) The Monitoring Query Language (MQL) expression to evaluate for the alarm. The Alarms feature of the Monitoring service interprets results for each returned time series as Boolean values, where zero represents false and a non-zero value represents true. A true value means that the trigger rule condition has been met. The query must specify a metric, statistic, interval, and trigger rule (threshold or absence). Supported values for interval: 1m-60m (also 1h). You can optionally specify dimensions and grouping functions. Supported grouping functions: grouping(), groupBy().
	Query *string `json:"query,omitempty" tf:"query,omitempty"`

	// (Updatable) The type of scaling configuration.
	ScalingConfigurationType *string `json:"scalingConfigurationType,omitempty" tf:"scaling_configuration_type,omitempty"`

	// (Updatable) A metric value at which the scaling operation will be triggered.
	Threshold *float64 `json:"threshold,omitempty" tf:"threshold,omitempty"`
}

type RulesScaleOutConfigurationObservation struct {

	// (Applicable when metric_expression_rule_type=CUSTOM_EXPRESSION | PREDEFINED_EXPRESSION) (Updatable) The value is used for adjusting the count of instances by.
	InstanceCountAdjustment *float64 `json:"instanceCountAdjustment,omitempty" tf:"instance_count_adjustment,omitempty"`

	// (Applicable when metric_expression_rule_type=CUSTOM_EXPRESSION | PREDEFINED_EXPRESSION) (Updatable) The period of time that the condition defined in the alarm must persist before the alarm state changes from "OK" to "FIRING" or vice versa. For example, a value of 5 minutes means that the alarm must persist in breaching the condition for five minutes before the alarm updates its state to "FIRING"; likewise, the alarm must persist in not breaching the condition for five minutes before the alarm updates its state to "OK."
	PendingDuration *string `json:"pendingDuration,omitempty" tf:"pending_duration,omitempty"`

	// (Updatable) The Monitoring Query Language (MQL) expression to evaluate for the alarm. The Alarms feature of the Monitoring service interprets results for each returned time series as Boolean values, where zero represents false and a non-zero value represents true. A true value means that the trigger rule condition has been met. The query must specify a metric, statistic, interval, and trigger rule (threshold or absence). Supported values for interval: 1m-60m (also 1h). You can optionally specify dimensions and grouping functions. Supported grouping functions: grouping(), groupBy().
	Query *string `json:"query,omitempty" tf:"query,omitempty"`

	// (Updatable) The type of scaling configuration.
	ScalingConfigurationType *string `json:"scalingConfigurationType,omitempty" tf:"scaling_configuration_type,omitempty"`

	// (Updatable) A metric value at which the scaling operation will be triggered.
	Threshold *float64 `json:"threshold,omitempty" tf:"threshold,omitempty"`
}

type RulesScaleOutConfigurationParameters struct {

	// (Applicable when metric_expression_rule_type=CUSTOM_EXPRESSION | PREDEFINED_EXPRESSION) (Updatable) The value is used for adjusting the count of instances by.
	// +kubebuilder:validation:Optional
	InstanceCountAdjustment *float64 `json:"instanceCountAdjustment,omitempty" tf:"instance_count_adjustment,omitempty"`

	// (Applicable when metric_expression_rule_type=CUSTOM_EXPRESSION | PREDEFINED_EXPRESSION) (Updatable) The period of time that the condition defined in the alarm must persist before the alarm state changes from "OK" to "FIRING" or vice versa. For example, a value of 5 minutes means that the alarm must persist in breaching the condition for five minutes before the alarm updates its state to "FIRING"; likewise, the alarm must persist in not breaching the condition for five minutes before the alarm updates its state to "OK."
	// +kubebuilder:validation:Optional
	PendingDuration *string `json:"pendingDuration,omitempty" tf:"pending_duration,omitempty"`

	// (Updatable) The Monitoring Query Language (MQL) expression to evaluate for the alarm. The Alarms feature of the Monitoring service interprets results for each returned time series as Boolean values, where zero represents false and a non-zero value represents true. A true value means that the trigger rule condition has been met. The query must specify a metric, statistic, interval, and trigger rule (threshold or absence). Supported values for interval: 1m-60m (also 1h). You can optionally specify dimensions and grouping functions. Supported grouping functions: grouping(), groupBy().
	// +kubebuilder:validation:Optional
	Query *string `json:"query,omitempty" tf:"query,omitempty"`

	// (Updatable) The type of scaling configuration.
	// +kubebuilder:validation:Optional
	ScalingConfigurationType *string `json:"scalingConfigurationType,omitempty" tf:"scaling_configuration_type,omitempty"`

	// (Updatable) A metric value at which the scaling operation will be triggered.
	// +kubebuilder:validation:Optional
	Threshold *float64 `json:"threshold,omitempty" tf:"threshold,omitempty"`
}

type ScaleInConfigurationInitParameters struct {

	// (Applicable when metric_expression_rule_type=CUSTOM_EXPRESSION | PREDEFINED_EXPRESSION) (Updatable) The value is used for adjusting the count of instances by.
	InstanceCountAdjustment *float64 `json:"instanceCountAdjustment,omitempty" tf:"instance_count_adjustment,omitempty"`

	// (Applicable when metric_expression_rule_type=CUSTOM_EXPRESSION | PREDEFINED_EXPRESSION) (Updatable) The period of time that the condition defined in the alarm must persist before the alarm state changes from "OK" to "FIRING" or vice versa. For example, a value of 5 minutes means that the alarm must persist in breaching the condition for five minutes before the alarm updates its state to "FIRING"; likewise, the alarm must persist in not breaching the condition for five minutes before the alarm updates its state to "OK."
	PendingDuration *string `json:"pendingDuration,omitempty" tf:"pending_duration,omitempty"`

	// (Updatable) The Monitoring Query Language (MQL) expression to evaluate for the alarm. The Alarms feature of the Monitoring service interprets results for each returned time series as Boolean values, where zero represents false and a non-zero value represents true. A true value means that the trigger rule condition has been met. The query must specify a metric, statistic, interval, and trigger rule (threshold or absence). Supported values for interval: 1m-60m (also 1h). You can optionally specify dimensions and grouping functions. Supported grouping functions: grouping(), groupBy().
	Query *string `json:"query,omitempty" tf:"query,omitempty"`

	// (Updatable) The type of scaling configuration.
	ScalingConfigurationType *string `json:"scalingConfigurationType,omitempty" tf:"scaling_configuration_type,omitempty"`

	// (Updatable) A metric value at which the scaling operation will be triggered.
	Threshold *float64 `json:"threshold,omitempty" tf:"threshold,omitempty"`
}

type ScaleInConfigurationObservation struct {

	// (Applicable when metric_expression_rule_type=CUSTOM_EXPRESSION | PREDEFINED_EXPRESSION) (Updatable) The value is used for adjusting the count of instances by.
	InstanceCountAdjustment *float64 `json:"instanceCountAdjustment,omitempty" tf:"instance_count_adjustment,omitempty"`

	// (Applicable when metric_expression_rule_type=CUSTOM_EXPRESSION | PREDEFINED_EXPRESSION) (Updatable) The period of time that the condition defined in the alarm must persist before the alarm state changes from "OK" to "FIRING" or vice versa. For example, a value of 5 minutes means that the alarm must persist in breaching the condition for five minutes before the alarm updates its state to "FIRING"; likewise, the alarm must persist in not breaching the condition for five minutes before the alarm updates its state to "OK."
	PendingDuration *string `json:"pendingDuration,omitempty" tf:"pending_duration,omitempty"`

	// (Updatable) The Monitoring Query Language (MQL) expression to evaluate for the alarm. The Alarms feature of the Monitoring service interprets results for each returned time series as Boolean values, where zero represents false and a non-zero value represents true. A true value means that the trigger rule condition has been met. The query must specify a metric, statistic, interval, and trigger rule (threshold or absence). Supported values for interval: 1m-60m (also 1h). You can optionally specify dimensions and grouping functions. Supported grouping functions: grouping(), groupBy().
	Query *string `json:"query,omitempty" tf:"query,omitempty"`

	// (Updatable) The type of scaling configuration.
	ScalingConfigurationType *string `json:"scalingConfigurationType,omitempty" tf:"scaling_configuration_type,omitempty"`

	// (Updatable) A metric value at which the scaling operation will be triggered.
	Threshold *float64 `json:"threshold,omitempty" tf:"threshold,omitempty"`
}

type ScaleInConfigurationParameters struct {

	// (Applicable when metric_expression_rule_type=CUSTOM_EXPRESSION | PREDEFINED_EXPRESSION) (Updatable) The value is used for adjusting the count of instances by.
	// +kubebuilder:validation:Optional
	InstanceCountAdjustment *float64 `json:"instanceCountAdjustment,omitempty" tf:"instance_count_adjustment,omitempty"`

	// (Applicable when metric_expression_rule_type=CUSTOM_EXPRESSION | PREDEFINED_EXPRESSION) (Updatable) The period of time that the condition defined in the alarm must persist before the alarm state changes from "OK" to "FIRING" or vice versa. For example, a value of 5 minutes means that the alarm must persist in breaching the condition for five minutes before the alarm updates its state to "FIRING"; likewise, the alarm must persist in not breaching the condition for five minutes before the alarm updates its state to "OK."
	// +kubebuilder:validation:Optional
	PendingDuration *string `json:"pendingDuration,omitempty" tf:"pending_duration,omitempty"`

	// (Updatable) The Monitoring Query Language (MQL) expression to evaluate for the alarm. The Alarms feature of the Monitoring service interprets results for each returned time series as Boolean values, where zero represents false and a non-zero value represents true. A true value means that the trigger rule condition has been met. The query must specify a metric, statistic, interval, and trigger rule (threshold or absence). Supported values for interval: 1m-60m (also 1h). You can optionally specify dimensions and grouping functions. Supported grouping functions: grouping(), groupBy().
	// +kubebuilder:validation:Optional
	Query *string `json:"query,omitempty" tf:"query,omitempty"`

	// (Updatable) The type of scaling configuration.
	// +kubebuilder:validation:Optional
	ScalingConfigurationType *string `json:"scalingConfigurationType,omitempty" tf:"scaling_configuration_type,omitempty"`

	// (Updatable) A metric value at which the scaling operation will be triggered.
	// +kubebuilder:validation:Optional
	Threshold *float64 `json:"threshold,omitempty" tf:"threshold,omitempty"`
}

type ScaleOutConfigurationInitParameters struct {

	// (Applicable when metric_expression_rule_type=CUSTOM_EXPRESSION | PREDEFINED_EXPRESSION) (Updatable) The value is used for adjusting the count of instances by.
	InstanceCountAdjustment *float64 `json:"instanceCountAdjustment,omitempty" tf:"instance_count_adjustment,omitempty"`

	// (Applicable when metric_expression_rule_type=CUSTOM_EXPRESSION | PREDEFINED_EXPRESSION) (Updatable) The period of time that the condition defined in the alarm must persist before the alarm state changes from "OK" to "FIRING" or vice versa. For example, a value of 5 minutes means that the alarm must persist in breaching the condition for five minutes before the alarm updates its state to "FIRING"; likewise, the alarm must persist in not breaching the condition for five minutes before the alarm updates its state to "OK."
	PendingDuration *string `json:"pendingDuration,omitempty" tf:"pending_duration,omitempty"`

	// (Updatable) The Monitoring Query Language (MQL) expression to evaluate for the alarm. The Alarms feature of the Monitoring service interprets results for each returned time series as Boolean values, where zero represents false and a non-zero value represents true. A true value means that the trigger rule condition has been met. The query must specify a metric, statistic, interval, and trigger rule (threshold or absence). Supported values for interval: 1m-60m (also 1h). You can optionally specify dimensions and grouping functions. Supported grouping functions: grouping(), groupBy().
	Query *string `json:"query,omitempty" tf:"query,omitempty"`

	// (Updatable) The type of scaling configuration.
	ScalingConfigurationType *string `json:"scalingConfigurationType,omitempty" tf:"scaling_configuration_type,omitempty"`

	// (Updatable) A metric value at which the scaling operation will be triggered.
	Threshold *float64 `json:"threshold,omitempty" tf:"threshold,omitempty"`
}

type ScaleOutConfigurationObservation struct {

	// (Applicable when metric_expression_rule_type=CUSTOM_EXPRESSION | PREDEFINED_EXPRESSION) (Updatable) The value is used for adjusting the count of instances by.
	InstanceCountAdjustment *float64 `json:"instanceCountAdjustment,omitempty" tf:"instance_count_adjustment,omitempty"`

	// (Applicable when metric_expression_rule_type=CUSTOM_EXPRESSION | PREDEFINED_EXPRESSION) (Updatable) The period of time that the condition defined in the alarm must persist before the alarm state changes from "OK" to "FIRING" or vice versa. For example, a value of 5 minutes means that the alarm must persist in breaching the condition for five minutes before the alarm updates its state to "FIRING"; likewise, the alarm must persist in not breaching the condition for five minutes before the alarm updates its state to "OK."
	PendingDuration *string `json:"pendingDuration,omitempty" tf:"pending_duration,omitempty"`

	// (Updatable) The Monitoring Query Language (MQL) expression to evaluate for the alarm. The Alarms feature of the Monitoring service interprets results for each returned time series as Boolean values, where zero represents false and a non-zero value represents true. A true value means that the trigger rule condition has been met. The query must specify a metric, statistic, interval, and trigger rule (threshold or absence). Supported values for interval: 1m-60m (also 1h). You can optionally specify dimensions and grouping functions. Supported grouping functions: grouping(), groupBy().
	Query *string `json:"query,omitempty" tf:"query,omitempty"`

	// (Updatable) The type of scaling configuration.
	ScalingConfigurationType *string `json:"scalingConfigurationType,omitempty" tf:"scaling_configuration_type,omitempty"`

	// (Updatable) A metric value at which the scaling operation will be triggered.
	Threshold *float64 `json:"threshold,omitempty" tf:"threshold,omitempty"`
}

type ScaleOutConfigurationParameters struct {

	// (Applicable when metric_expression_rule_type=CUSTOM_EXPRESSION | PREDEFINED_EXPRESSION) (Updatable) The value is used for adjusting the count of instances by.
	// +kubebuilder:validation:Optional
	InstanceCountAdjustment *float64 `json:"instanceCountAdjustment,omitempty" tf:"instance_count_adjustment,omitempty"`

	// (Applicable when metric_expression_rule_type=CUSTOM_EXPRESSION | PREDEFINED_EXPRESSION) (Updatable) The period of time that the condition defined in the alarm must persist before the alarm state changes from "OK" to "FIRING" or vice versa. For example, a value of 5 minutes means that the alarm must persist in breaching the condition for five minutes before the alarm updates its state to "FIRING"; likewise, the alarm must persist in not breaching the condition for five minutes before the alarm updates its state to "OK."
	// +kubebuilder:validation:Optional
	PendingDuration *string `json:"pendingDuration,omitempty" tf:"pending_duration,omitempty"`

	// (Updatable) The Monitoring Query Language (MQL) expression to evaluate for the alarm. The Alarms feature of the Monitoring service interprets results for each returned time series as Boolean values, where zero represents false and a non-zero value represents true. A true value means that the trigger rule condition has been met. The query must specify a metric, statistic, interval, and trigger rule (threshold or absence). Supported values for interval: 1m-60m (also 1h). You can optionally specify dimensions and grouping functions. Supported grouping functions: grouping(), groupBy().
	// +kubebuilder:validation:Optional
	Query *string `json:"query,omitempty" tf:"query,omitempty"`

	// (Updatable) The type of scaling configuration.
	// +kubebuilder:validation:Optional
	ScalingConfigurationType *string `json:"scalingConfigurationType,omitempty" tf:"scaling_configuration_type,omitempty"`

	// (Updatable) A metric value at which the scaling operation will be triggered.
	// +kubebuilder:validation:Optional
	Threshold *float64 `json:"threshold,omitempty" tf:"threshold,omitempty"`
}

type ScalingPolicyAutoScalingPoliciesInitParameters struct {

	// (Updatable) The type of autoscaling policy.
	AutoScalingPolicyType *string `json:"autoScalingPolicyType,omitempty" tf:"auto_scaling_policy_type,omitempty"`

	// (Updatable) For a threshold-based autoscaling policy, this value is the initial number of instances to launch in the model deployment immediately after autoscaling is enabled. Note that anytime this value is updated, the number of instances will be reset to this value. After autoscaling retrieves performance metrics, the number of instances is automatically adjusted from this initial number to a number that is based on the limits that you set.
	InitialInstanceCount *float64 `json:"initialInstanceCount,omitempty" tf:"initial_instance_count,omitempty"`

	// (Updatable) For a threshold-based autoscaling policy, this value is the maximum number of instances the model deployment is allowed to increase to (scale out).
	MaximumInstanceCount *float64 `json:"maximumInstanceCount,omitempty" tf:"maximum_instance_count,omitempty"`

	// (Updatable) For a threshold-based autoscaling policy, this value is the minimum number of instances the model deployment is allowed to decrease to (scale in).
	MinimumInstanceCount *float64 `json:"minimumInstanceCount,omitempty" tf:"minimum_instance_count,omitempty"`

	// (Updatable) The list of autoscaling policy rules.
	Rules []AutoScalingPoliciesRulesInitParameters `json:"rules,omitempty" tf:"rules,omitempty"`
}

type ScalingPolicyAutoScalingPoliciesObservation struct {

	// (Updatable) The type of autoscaling policy.
	AutoScalingPolicyType *string `json:"autoScalingPolicyType,omitempty" tf:"auto_scaling_policy_type,omitempty"`

	// (Updatable) For a threshold-based autoscaling policy, this value is the initial number of instances to launch in the model deployment immediately after autoscaling is enabled. Note that anytime this value is updated, the number of instances will be reset to this value. After autoscaling retrieves performance metrics, the number of instances is automatically adjusted from this initial number to a number that is based on the limits that you set.
	InitialInstanceCount *float64 `json:"initialInstanceCount,omitempty" tf:"initial_instance_count,omitempty"`

	// (Updatable) For a threshold-based autoscaling policy, this value is the maximum number of instances the model deployment is allowed to increase to (scale out).
	MaximumInstanceCount *float64 `json:"maximumInstanceCount,omitempty" tf:"maximum_instance_count,omitempty"`

	// (Updatable) For a threshold-based autoscaling policy, this value is the minimum number of instances the model deployment is allowed to decrease to (scale in).
	MinimumInstanceCount *float64 `json:"minimumInstanceCount,omitempty" tf:"minimum_instance_count,omitempty"`

	// (Updatable) The list of autoscaling policy rules.
	Rules []AutoScalingPoliciesRulesObservation `json:"rules,omitempty" tf:"rules,omitempty"`
}

type ScalingPolicyAutoScalingPoliciesParameters struct {

	// (Updatable) The type of autoscaling policy.
	// +kubebuilder:validation:Optional
	AutoScalingPolicyType *string `json:"autoScalingPolicyType" tf:"auto_scaling_policy_type,omitempty"`

	// (Updatable) For a threshold-based autoscaling policy, this value is the initial number of instances to launch in the model deployment immediately after autoscaling is enabled. Note that anytime this value is updated, the number of instances will be reset to this value. After autoscaling retrieves performance metrics, the number of instances is automatically adjusted from this initial number to a number that is based on the limits that you set.
	// +kubebuilder:validation:Optional
	InitialInstanceCount *float64 `json:"initialInstanceCount" tf:"initial_instance_count,omitempty"`

	// (Updatable) For a threshold-based autoscaling policy, this value is the maximum number of instances the model deployment is allowed to increase to (scale out).
	// +kubebuilder:validation:Optional
	MaximumInstanceCount *float64 `json:"maximumInstanceCount" tf:"maximum_instance_count,omitempty"`

	// (Updatable) For a threshold-based autoscaling policy, this value is the minimum number of instances the model deployment is allowed to decrease to (scale in).
	// +kubebuilder:validation:Optional
	MinimumInstanceCount *float64 `json:"minimumInstanceCount" tf:"minimum_instance_count,omitempty"`

	// (Updatable) The list of autoscaling policy rules.
	// +kubebuilder:validation:Optional
	Rules []AutoScalingPoliciesRulesParameters `json:"rules" tf:"rules,omitempty"`
}

type ScalingPolicyInitParameters struct {

	// (Updatable) The list of autoscaling policy details.
	AutoScalingPolicies []AutoScalingPoliciesInitParameters `json:"autoScalingPolicies,omitempty" tf:"auto_scaling_policies,omitempty"`

	// (Applicable when policy_type=AUTOSCALING) (Updatable) For threshold-based autoscaling policies, this value is the minimum period of time to wait between scaling actions. The cooldown period gives the system time to stabilize before rescaling. The minimum value is 600 seconds, which is also the default. The cooldown period starts when the model deployment becomes ACTIVE after the scaling operation.
	CoolDownInSeconds *float64 `json:"coolDownInSeconds,omitempty" tf:"cool_down_in_seconds,omitempty"`

	// (Updatable) The number of instances for the model deployment.
	InstanceCount *float64 `json:"instanceCount,omitempty" tf:"instance_count,omitempty"`

	// (Applicable when policy_type=AUTOSCALING) (Updatable) Whether the autoscaling policy is enabled.
	IsEnabled *bool `json:"isEnabled,omitempty" tf:"is_enabled,omitempty"`

	// (Updatable) The type of scaling policy.
	PolicyType *string `json:"policyType,omitempty" tf:"policy_type,omitempty"`
}

type ScalingPolicyObservation struct {

	// (Updatable) The list of autoscaling policy details.
	AutoScalingPolicies []AutoScalingPoliciesObservation `json:"autoScalingPolicies,omitempty" tf:"auto_scaling_policies,omitempty"`

	// (Applicable when policy_type=AUTOSCALING) (Updatable) For threshold-based autoscaling policies, this value is the minimum period of time to wait between scaling actions. The cooldown period gives the system time to stabilize before rescaling. The minimum value is 600 seconds, which is also the default. The cooldown period starts when the model deployment becomes ACTIVE after the scaling operation.
	CoolDownInSeconds *float64 `json:"coolDownInSeconds,omitempty" tf:"cool_down_in_seconds,omitempty"`

	// (Updatable) The number of instances for the model deployment.
	InstanceCount *float64 `json:"instanceCount,omitempty" tf:"instance_count,omitempty"`

	// (Applicable when policy_type=AUTOSCALING) (Updatable) Whether the autoscaling policy is enabled.
	IsEnabled *bool `json:"isEnabled,omitempty" tf:"is_enabled,omitempty"`

	// (Updatable) The type of scaling policy.
	PolicyType *string `json:"policyType,omitempty" tf:"policy_type,omitempty"`
}

type ScalingPolicyParameters struct {

	// (Updatable) The list of autoscaling policy details.
	// +kubebuilder:validation:Optional
	AutoScalingPolicies []AutoScalingPoliciesParameters `json:"autoScalingPolicies,omitempty" tf:"auto_scaling_policies,omitempty"`

	// (Applicable when policy_type=AUTOSCALING) (Updatable) For threshold-based autoscaling policies, this value is the minimum period of time to wait between scaling actions. The cooldown period gives the system time to stabilize before rescaling. The minimum value is 600 seconds, which is also the default. The cooldown period starts when the model deployment becomes ACTIVE after the scaling operation.
	// +kubebuilder:validation:Optional
	CoolDownInSeconds *float64 `json:"coolDownInSeconds,omitempty" tf:"cool_down_in_seconds,omitempty"`

	// (Updatable) The number of instances for the model deployment.
	// +kubebuilder:validation:Optional
	InstanceCount *float64 `json:"instanceCount,omitempty" tf:"instance_count,omitempty"`

	// (Applicable when policy_type=AUTOSCALING) (Updatable) Whether the autoscaling policy is enabled.
	// +kubebuilder:validation:Optional
	IsEnabled *bool `json:"isEnabled,omitempty" tf:"is_enabled,omitempty"`

	// (Updatable) The type of scaling policy.
	// +kubebuilder:validation:Optional
	PolicyType *string `json:"policyType" tf:"policy_type,omitempty"`
}

// ModelDeploymentSpec defines the desired state of ModelDeployment
type ModelDeploymentSpec struct {
	v1.ResourceSpec `json:",inline"`
	ForProvider     ModelDeploymentParameters `json:"forProvider"`
	// THIS IS A BETA FIELD. It will be honored
	// unless the Management Policies feature flag is disabled.
	// InitProvider holds the same fields as ForProvider, with the exception
	// of Identifier and other resource reference fields. The fields that are
	// in InitProvider are merged into ForProvider when the resource is created.
	// The same fields are also added to the terraform ignore_changes hook, to
	// avoid updating them after creation. This is useful for fields that are
	// required on creation, but we do not desire to update them after creation,
	// for example because of an external controller is managing them, like an
	// autoscaler.
	InitProvider ModelDeploymentInitParameters `json:"initProvider,omitempty"`
}

// ModelDeploymentStatus defines the observed state of ModelDeployment.
type ModelDeploymentStatus struct {
	v1.ResourceStatus `json:",inline"`
	AtProvider        ModelDeploymentObservation `json:"atProvider,omitempty"`
}

// +kubebuilder:object:root=true
// +kubebuilder:subresource:status
// +kubebuilder:storageversion

// ModelDeployment is the Schema for the ModelDeployments API. Provides the Model Deployment resource in Oracle Cloud Infrastructure Data Science service
// +kubebuilder:printcolumn:name="SYNCED",type="string",JSONPath=".status.conditions[?(@.type=='Synced')].status"
// +kubebuilder:printcolumn:name="READY",type="string",JSONPath=".status.conditions[?(@.type=='Ready')].status"
// +kubebuilder:printcolumn:name="EXTERNAL-NAME",type="string",JSONPath=".metadata.annotations.crossplane\\.io/external-name"
// +kubebuilder:printcolumn:name="AGE",type="date",JSONPath=".metadata.creationTimestamp"
// +kubebuilder:resource:scope=Cluster,categories={crossplane,managed,oci}
type ModelDeployment struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`
	// +kubebuilder:validation:XValidation:rule="!('*' in self.managementPolicies || 'Create' in self.managementPolicies || 'Update' in self.managementPolicies) || has(self.forProvider.compartmentId) || (has(self.initProvider) && has(self.initProvider.compartmentId))",message="spec.forProvider.compartmentId is a required parameter"
	// +kubebuilder:validation:XValidation:rule="!('*' in self.managementPolicies || 'Create' in self.managementPolicies || 'Update' in self.managementPolicies) || has(self.forProvider.modelDeploymentConfigurationDetails) || (has(self.initProvider) && has(self.initProvider.modelDeploymentConfigurationDetails))",message="spec.forProvider.modelDeploymentConfigurationDetails is a required parameter"
	// +kubebuilder:validation:XValidation:rule="!('*' in self.managementPolicies || 'Create' in self.managementPolicies || 'Update' in self.managementPolicies) || has(self.forProvider.projectId) || (has(self.initProvider) && has(self.initProvider.projectId))",message="spec.forProvider.projectId is a required parameter"
	Spec   ModelDeploymentSpec   `json:"spec"`
	Status ModelDeploymentStatus `json:"status,omitempty"`
}

// +kubebuilder:object:root=true

// ModelDeploymentList contains a list of ModelDeployments
type ModelDeploymentList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []ModelDeployment `json:"items"`
}

// Repository type metadata.
var (
	ModelDeployment_Kind             = "ModelDeployment"
	ModelDeployment_GroupKind        = schema.GroupKind{Group: CRDGroup, Kind: ModelDeployment_Kind}.String()
	ModelDeployment_KindAPIVersion   = ModelDeployment_Kind + "." + CRDGroupVersion.String()
	ModelDeployment_GroupVersionKind = CRDGroupVersion.WithKind(ModelDeployment_Kind)
)

func init() {
	SchemeBuilder.Register(&ModelDeployment{}, &ModelDeploymentList{})
}
